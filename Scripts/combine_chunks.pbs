#!/bin/bash
#PBS -N combine_chunks
#PBS -l select=1:ncpus=1:mem=4gb
#PBS -l walltime=00:10:00
#PBS -j oe
#PBS -V
#PBS -o logs/
#PBS -e logs/

# Load environment
module load SACLA_tool
source ~/venv/bin/activate

# Move to working directory
cd $PBS_O_WORKDIR

# Input parameters passed via -v
INPUT_DIR=${INPUT_DIR}
RUN_NUMBER=${RUN_NUMBER}
OUTPUT_FILE=${OUTPUT_FILE}
DATA_PATH=${DATA_PATH}

echo "=== PBS script started ===" > $PBS_O_WORKDIR/logs/pbs_debug.log

# Combine all chunks
python <<EOF
import os
import h5py
import numpy as np
import glob

input_dir = os.environ["INPUT_DIR"]
run_number = os.environ["RUN_NUMBER"]
output_file = os.environ["OUTPUT_FILE"]
data_path = os.environ["DATA_PATH"]

# === Locate and sort chunk files ===
pattern = os.path.join(input_dir, f"Iq_{run_number}_chunk*.h5")
files = sorted(glob.glob(pattern))

if not files:
    print(f"No chunk files found matching pattern: {pattern}")
    exit(1)

# === Concatenate I, q, phi, and image_id ===
I_list, image_id_list, q, phi = [], [], None, None

for fname in files:
    with h5py.File(fname, "r") as f:
        I_list.append(f["I"][:])
        image_id_list.append(f["image_id"][:])
        if q is None:
            q = f["q"][:]
        if phi is None:
            phi = f["phi"][:]

I_all = np.concatenate(I_list, axis=0)
image_ids = np.concatenate(image_id_list)

# === Read metadata from result.log ===
#result_log_path = #os.path.join(os.path.dirname(input_dir), run_number, #"result.log")
#meta = {}

#if os.path.isfile(result_log_path):
#    with open(result_log_path, "r") as f:
#        line = f.readline().strip()
#        for entry in line.split(","):
#            key, val = entry.strip().split(":")
#            meta[key] = int(val)
#else:
#    print(f"WARNING: result.log not found at #{result_log_path}")

meta = {}
raw_data_dir = os.path.join(data_path, run_number)
log_file = os.path.join(raw_data_dir, "result.log")
if os.path.exists(log_file):
    with open(log_file, "r") as f:
        line = f.readline().strip()
        for item in line.split(","):
            key, val = item.split(":")
            meta[key.strip()] = val.strip()
else:
    print(f"Metadata file not found: {log_file}")


# === Write combined data and metadata ===
with h5py.File(output_file, "w") as f:
    f.create_dataset("I", data=I_all)
    f.create_dataset("q", data=q)
    f.create_dataset("phi", data=phi)
    f.create_dataset("image_id", data=image_ids)

    # Write metadata
    for key, val in meta.items():
        try:
            f.create_dataset(key, data=int(val))
        except ValueError:
            f.create_dataset(key, data=str(val))
    #for key, val in meta.items():
    #    f.create_dataset(key, data=val)

print(f"Combined data and metadata saved to {output_file}")
EOF

# Delete chunk files if the merged file was created
if [ -f "$OUTPUT_FILE" ]; then
    echo "Merged file created: $OUTPUT_FILE"
    echo "Deleting chunk files..."
    rm -v "${INPUT_DIR}/Iq_${RUN_NUMBER}_chunk"*.h5
else
    echo "Merged file not found. Skipping chunk deletion."
fi
